{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 M√°quinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducci√≥n a M√°quinas de Aprendizaje </H3>\n",
    "<H3 align='center'> Integrantes: Roberto Calfulef - Matias Moreno </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducci√≥n a librer√≠as comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementaci√≥n de Perceptr√≥n y variantes.\n",
    "* Implementaci√≥n de m√©todo aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 6 de Septiembre.\n",
    "* Formato de entrega: envƒ±ÃÅo de link Github al correo electr√≥nico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptr√≥n a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#librerias\n",
    "import numpy.matlib \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Escriba una funci√≥n que calcule el valor de salida (output) del modelo f(x) para un patr√≥n de entrada x a trav√©s de los pesos w del modelo. Decida si incluir los bias dentro de w o manejarlos de manera separada.\n",
    "#Input:\n",
    "# X = vector de caracteristicas\n",
    "# W = vector de pesos\n",
    "# b = bias\n",
    "\n",
    "#Output:\n",
    "# 0 o 1 \n",
    "\n",
    "def perceptron(X,W,b):\n",
    "    dot =  np.dot(X,W) - b\n",
    "    if(dot > 0):\n",
    "        return 1\n",
    "    return 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Escriba una funci√≥n que implemente el cl√°sico algoritmo del Perceptr√≥n para un problema binario que permita entrenarlo en un conjunto de datos de tama√±o N, le√≠dos de manera online (uno a uno). Recordar la decisi√≥n anterior sobre los bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input:\n",
    "# X = conjunto de entrada\n",
    "# Y = conjunto de salida\n",
    "# W = vector de pesos inicial\n",
    "# b = bias\n",
    "\n",
    "#Output:\n",
    "# 0 o 1 \n",
    "\n",
    "def perceptron_clasico(X,Y,W,b):\n",
    "    i=0\n",
    "    for vector in X:\n",
    "        resultado = perceptron(vector,W,b)\n",
    "        error = Y[i]-resultado\n",
    "        if error != 0:\n",
    "            W += np.multiply(vector,error*.1)\n",
    "            #print(W)\n",
    "        i = i+1\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto:  96.46017699115043 %\n"
     ]
    }
   ],
   "source": [
    "# c) Demuestre que lo implementado funciona en un problema real de clasificaci√≥n. Para esto utilice el dataset Breast cancer wisconsin, disponible a trav√©s de la librer√≠a sklearn, el cual corresponde a la detecci√≥n de cancer mamario a trav√©s de caracter√≠sticas relevantes (num√©ricas continuas) de un examen realizado, como por ejemplo la textura, simetr√≠a y tama√±o de una masa mamaria. Estas caracter√≠sticas deben combinarse linealmente para la detecci√≥n del cancer.\n",
    "from random import randrange\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "#X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "\n",
    "random = set()\n",
    "n_datos = len(y_train)\n",
    "#n_datos.add(elemento)\n",
    "while len(random) < 0.8*n_datos:\n",
    "    random.add(randrange(n_datos))\n",
    "    \n",
    "X_train80 = list()\n",
    "X_train20 = list()\n",
    "Y80 = list()\n",
    "Y20 = list()\n",
    "\n",
    "#Separo el set de datos en proporcion 80 y 20\n",
    "for j in range(len(X_train)):\n",
    "    if j in random:\n",
    "        X_train80.append(X_train[j])\n",
    "        Y80.append(y_train[j])\n",
    "    else:\n",
    "        X_train20.append(X_train[j])\n",
    "        Y20.append(y_train[j])\n",
    "        \n",
    "pesos = np.zeros(len(X_train[0]))\n",
    "Resultado = perceptron_clasico(X_train80,Y80,pesos,0)\n",
    "\n",
    "aciertos = 0\n",
    "\n",
    "for i in range(len(Y20)):\n",
    "    test = perceptron(X_train20[i],Resultado,0)\n",
    "    if Y20[i] == test:\n",
    "        aciertos = aciertos+1\n",
    "\n",
    "print (\"Porcentaje de acierto: \",aciertos/len(Y20)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar el algoritmo, el porcentaje de acierto ronda entre 93% a 95% con un training set de 80% de los datos elegidos aleatoriamente. Lo cual podriamos decir que es un clasificador decente y tiene pocos errores, por lo tanto el algoritmo hace su trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> d) Escriba una funci√≥n que implemente el **Forgetr√≥n** [[3]](#refs) con una memoria de tama√±o $K$ y la funci√≥n de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje de acierto:  98.23008849557522 %\n"
     ]
    }
   ],
   "source": [
    "#Creo una Cola tipo FIFO de tama√±o K\n",
    "from collections import deque\n",
    "K = 10\n",
    "#K_memory = collections.deque(K*[0], K)\n",
    "K_memory = deque(K*[-1], K)\n",
    "\n",
    "def forgetron(X,W,b):\n",
    "    suma = 0\n",
    "    for i in K_memory:\n",
    "        if i>=0:\n",
    "            suma = suma + Y80[i]*np.dot(X,X_train80[i])\n",
    "    if(suma > 0):\n",
    "        return 1\n",
    "    return 0 \n",
    "\n",
    "def perceptron_forgetron(X,Y,W,b):\n",
    "    i=0\n",
    "    for indice in range(len(X)):\n",
    "        resultado = forgetron(X[indice],W,b)\n",
    "        error = Y[i]-resultado\n",
    "        if error != 0:\n",
    "            W += np.multiply(X[indice],error*.1)\n",
    "            K_memory.appendleft(indice)\n",
    "        i = i+1\n",
    "    return W\n",
    "\n",
    "pesos = np.zeros(len(X_train[0]))\n",
    "Resultado = perceptron_forgetron(X_train80,Y80,pesos,0)\n",
    "\n",
    "aciertos = 0\n",
    "\n",
    "for i in range(len(Y20)):\n",
    "    test = perceptron(X_train20[i],Resultado,0)\n",
    "    if Y20[i] == test:\n",
    "        aciertos = aciertos+1\n",
    "\n",
    "print (\"Porcentaje de acierto: \",aciertos/len(Y20)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Usando el algoritmo modificado, el Forgetr√≥n. El porcentaje de acierto sube notablemente, de 95 a un 98%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# f) Escriba una funci√≥n que compute la funci√≥n sigmoidal para una entrada  ùúâ  cualquiera. \n",
    "#Tenga cuidado con los l√≠mites de n√∫meros que puede trabajar python (por ejemplo  exp800‚Üí+‚àû ). \n",
    "#Se aconseja acotar/truncar los valores que entran a la funci√≥n para que la operaci√≥n se pueda realizar. \n",
    "#Adem√°s escriba una funci√≥n que calcule la salida del nuevo modelo  ùëî(ùë•;ùë§,ùëè)  con esta funci√≥n sigmoidal.\n",
    "\n",
    "def sigmoidal(E):\n",
    "    if E > 800:\n",
    "        return 1\n",
    "    elif E < -800:\n",
    "        return 0\n",
    "    return 1/(1+np.exp(-1*E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_continuo(X,W,b,E):\n",
    "    res = sigmoidal(E)*(np.dot(X,W)-b)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  g) Escriba una funci√≥n que calcule la funci√≥n de p√©rdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los l√≠mites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*.\n",
    "$$\n",
    "\\ell (y, \\ g(x;w,b)) = - y \\cdot \\log{(g(x;w,b))} - (1-y) \\cdot \\log{(1-g(x;w,b))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perdida(X,W,b,E,y):\n",
    "    temp= perceptron_continuo(X,W,b,E)\n",
    "    if temp == 0:\n",
    "        return 1\n",
    "    return - y * np.log(temp) - (1-y)*np.log(1-temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptr√≥n a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta secci√≥n se le pedir√° que implemente el algoritmo online del *perceptr√≥n* [[2]](#refs) para aprender una funci√≥n de separaci√≥n lineal en un problema de clasificaci√≥n binaria (0 o 1) a trav√©s de la funci√≥n de *treshold*. Un algoritmo online, como el caso del *perceptr√≥n*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicci√≥n de la clase para cada instancia es trav√©s de la funci√≥n de *treshold*:\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = f(x^{(i)};w,b) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir dentro de los pesos/par√°metros $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad s√≥lo podr√° utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n",
    "> a) Escriba una funci√≥n que calcule el valor de salida (*output*) del modelo $f(x)$ para un patr√≥n de entrada $x$ a trav√©s de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*.\n",
    "\n",
    "> b) Escriba una funci√≥n que implemente el cl√°sico algoritmo del **Perceptr√≥n** para un problema binario que permita entrenarlo en un conjunto de datos de tama√±o $N$, le√≠dos de manera *online* (uno a uno). *Recordar la decisi√≥n anterior sobre los bias*.\n",
    "\n",
    "> c) Demuestre que lo implementado funciona en un problema real de clasificaci√≥n. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a trav√©s de la librer√≠a __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detecci√≥n de cancer mamario a trav√©s de caracter√≠sticas relevantes (num√©ricas continuas) de un examen realizado, como por ejemplo la textura, simetr√≠a y tama√±o de una masa mamaria. Estas caracter√≠sticas deben combinarse linealmente para la detecci√≥n del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena pr√°ctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "```\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificaci√≥n durante el entrenamiento (por cada iteraci√≥n/instancia/dato) y grafique, utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta secci√≥n es familiarizarse con el algoritmo). Adem√°s reporte el tiempo de entrenamiento mediante el algoritmo implementado.\n",
    "\n",
    "> d) Escriba una funci√≥n que implemente el **Forgetr√≥n** [[3]](#refs) con una memoria de tama√±o $K$ y la funci√≥n de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$.\n",
    "\n",
    "> e) Vuelva a realizar el item c) para el **Forgetr√≥n** con un $K=10$ y compare los resultados.\n",
    "\n",
    "\n",
    "### ¬øQu√© sucede al variar la funci√≥n objetivo del problema? \n",
    "Si utiliz√°ramos la funci√≥n de p√©rdida *binary cross entropy*, que castiga de manera suave los valores en que se equivoca el modelo a trav√©s de que el valor de salida sea una confiabilidad $g(x; w,b) \\in [0,1]$.\n",
    "$$\n",
    "\\ell (y, \\ g(x;w,b)) = - y \\cdot \\log{(g(x;w,b))} - (1-y) \\cdot \\log{(1-g(x;w,b))}\n",
    "$$\n",
    "\n",
    "Realice una modificaci√≥n al perceptr√≥n para que entregue como salida una confiabilidad continua entre 0 y 1. Una buena aproximaci√≥n de la funci√≥n *treshold* (con $\\theta=0$) del perceptr√≥n es la funci√≥n sigmoidal.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lr6F3Ur.png\" width=\"60%\"  />\n",
    "\n",
    "√âsto ser√≠a modelar el perceptr√≥n como:\n",
    "$$\n",
    "g(x^{(i)};w,b) = p(y=1|x^{(i)}) = \\sigma \\left( \\sum_j w_j \\cdot x^{(i)}_j +b \\right)\n",
    "$$\n",
    "\n",
    "Con $\\sigma$ la funci√≥n sigmoidal de la forma $\\sigma(\\xi) = 1/(1+e^{-\\xi}) $, la cual tiene una derivada c√≠clica que hace m√°s f√°cil el c√°lculo: $\\sigma'(\\xi) = \\sigma(\\xi) (1-\\sigma(\\xi))$\n",
    "\n",
    "> f) Escriba una funci√≥n que compute la funci√≥n sigmoidal para una entrada $\\xi$ cualquiera. *Tenga cuidado con los l√≠mites de n√∫meros que puede trabajar python (por ejemplo $\\exp{800}\\rightarrow +\\infty$)*. *Se aconseja acotar/truncar los valores que entran a la funci√≥n para que la operaci√≥n se pueda realizar*. Adem√°s escriba una funci√≥n que calcule la salida del nuevo modelo $g(x; w,b)$ con esta funci√≥n sigmoidal.\n",
    "\n",
    "> g) Escriba una funci√≥n que calcule la funci√≥n de p√©rdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los l√≠mites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*.\n",
    "\n",
    "> h) Escriba una funci√≥n que calcule el gradiente (derivada) de la funci√≥n de p√©rdida anterior con respecto a los pesos del modelo $w$. *Se recomienda derivarla anal√≠ticamente y luego escribirla*. *Recuerde el uso de la regla de la cadena*.\n",
    "\n",
    "> i) Realice una modificaci√≥n al algoritmo implementado en b) (**Perceptr√≥n**) para que se adapte a la funci√≥n objetivo *binary cross entropy* implementada, para √©sto haga uso del algoritmo de optimizaci√≥n SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\cdot \\nabla_{\\vec{w}^{(t)}} \\ell $$\n",
    "\n",
    "> j) Vuelva a realizar el item c) con esta modificaci√≥n, adem√°s grafique la funci√≥n de p√©rdida en el transcurso del entrenamiento. Compare los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"refs\"></a>\n",
    "### Referencias\n",
    "[1] Hastie, T.; Tibshirani, R., Friedman, J. (2009), *The Elements of Statistical Learning*, Second Edition.\n",
    "Springer New York Inc.  \n",
    "[2] STEPHEN, I. (1990). *Perceptron-based learning algorithms*. IEEE Transactions on neural networks, 50(2), 179.  \n",
    "[3] Dekel, O., Shalev-Shwartz, S., & Singer, Y. (2006). *The Forgetron: A kernel-based perceptron on a fixed budget*. In Advances in neural information processing systems (pp. 259-266).  \n",
    "[4] Ruder, S. (2016). *An overview of gradient descent optimization algorithms*. arXiv preprint arXiv:1609.04747.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
